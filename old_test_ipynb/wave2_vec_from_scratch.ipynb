{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor, Wav2Vec2Model\n",
    "import torch as t\n",
    "import numpy as np\n",
    "t.cuda.set_device(8)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = \"/raid/amana/lavish_multi_model/emotion_detection/data/raw_audio/\"\n",
    "base_path = \"/raid/amana/lavish_multi_model/emotion_detection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh my God, hes lost it. Hes totally lost it.</td>\n",
       "      <td>Phoebe</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:20:57,256</td>\n",
       "      <td>00:21:00,049</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:21:01,927</td>\n",
       "      <td>00:21:03,261</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Or! Or, we could go to the bank, close our acc...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:24,660</td>\n",
       "      <td>00:12:30,915</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Youre a genius!</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:32,334</td>\n",
       "      <td>00:12:33,960</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Aww, man, now we wont be bank buddies!</td>\n",
       "      <td>Joey</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:34,211</td>\n",
       "      <td>00:12:37,505</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance   Speaker  \\\n",
       "0       1     Oh my God, hes lost it. Hes totally lost it.    Phoebe   \n",
       "1       2                                              What?    Monica   \n",
       "2       3  Or! Or, we could go to the bank, close our acc...      Ross   \n",
       "3       4                                   Youre a genius!  Chandler   \n",
       "4       5            Aww, man, now we wont be bank buddies!      Joey   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   sadness  negative            0             0       4        7   \n",
       "1  surprise  negative            0             1       4        7   \n",
       "2   neutral   neutral            1             0       4        4   \n",
       "3       joy  positive            1             1       4        4   \n",
       "4   sadness  negative            1             2       4        4   \n",
       "\n",
       "      StartTime       EndTime  \\\n",
       "0  00:20:57,256  00:21:00,049   \n",
       "1  00:21:01,927  00:21:03,261   \n",
       "2  00:12:24,660  00:12:30,915   \n",
       "3  00:12:32,334  00:12:33,960   \n",
       "4  00:12:34,211  00:12:37,505   \n",
       "\n",
       "                                            filename  \n",
       "0  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "1  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "2  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "3  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "4  /raid/amana/lavish_multi_model/emotion_detecti...  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/raid/amana/lavish_multi_model/emotion_detection/data/text_data.csv\")\n",
    "df['filename'] = [f'{audio_dir}dia{a}_utt{b}.wav' for a,b in zip(df['Dialogue_ID'],df['Utterance_ID'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: 1109\n",
      "Step 1: 1109\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Now, theres two reasons.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:38,466</td>\n",
       "      <td>00:12:39,841</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1119</td>\n",
       "      <td>Ugh, how can you even ask that question?!</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0:02:45,501</td>\n",
       "      <td>0:02:48,182</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>758</td>\n",
       "      <td>Yeah! You can hook it up to your TV</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>00:13:26,514</td>\n",
       "      <td>00:13:29,015</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Come on man!  Listen so uh, are you gonna sque...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>00:07:01,254</td>\n",
       "      <td>00:07:07,342</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1107</td>\n",
       "      <td>Yknow what Greg?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>anger</td>\n",
       "      <td>negative</td>\n",
       "      <td>105</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>00:22:03,488</td>\n",
       "      <td>00:22:04,947</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance   Speaker  \\\n",
       "0       6                          Now, theres two reasons.  Chandler   \n",
       "1    1119          Ugh, how can you even ask that question?!    Rachel   \n",
       "2     758                Yeah! You can hook it up to your TV    Rachel   \n",
       "3     104  Come on man!  Listen so uh, are you gonna sque...      Joey   \n",
       "4    1107                                  Yknow what Greg?    Monica   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   neutral   neutral            1             3       4        4   \n",
       "1  surprise  positive          107             3       6       20   \n",
       "2       joy  positive           71             4       7       13   \n",
       "3       joy  positive           10             3       5       20   \n",
       "4     anger  negative          105            10       8        4   \n",
       "\n",
       "      StartTime       EndTime  \\\n",
       "0  00:12:38,466  00:12:39,841   \n",
       "1   0:02:45,501   0:02:48,182   \n",
       "2  00:13:26,514  00:13:29,015   \n",
       "3  00:07:01,254  00:07:07,342   \n",
       "4  00:22:03,488  00:22:04,947   \n",
       "\n",
       "                                            filename  \n",
       "0  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "1  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "2  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "3  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "4  /raid/amana/lavish_multi_model/emotion_detecti...  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step 0: {len(df)}\")\n",
    "df[\"status\"] = df[\"filename\"].apply(lambda path: True if os.path.exists(path) else None)\n",
    "df = df.dropna(subset=[\"filename\"])\n",
    "df = df.drop(\"status\", 1)\n",
    "print(f\"Step 1: {len(df)}\")\n",
    "df = df.sample(frac=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['neutral' 'surprise' 'joy' 'anger' 'fear' 'sadness' 'disgust']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename\n",
       "Emotion           \n",
       "anger          153\n",
       "disgust         22\n",
       "fear            40\n",
       "joy            163\n",
       "neutral        470\n",
       "sadness        111\n",
       "surprise       150"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Labels: \", df[\"Emotion\"].unique())\n",
    "print()\n",
    "df.groupby(\"Emotion\").count()[[\"filename\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 13)\n",
      "(222, 13)\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/raid/amana/lavish_multi_model/emotion_detection/data/\"\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"Emotion\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "random_float_array = np.random.uniform(7, 10.0, len(train_df.Emotion))\n",
    "train_df[\"emotion\"] = train_df['Emotion']\n",
    "train_df['Emotion'] = random_float_array.round(2)\n",
    "train_df = train_df.rename(columns={\"Emotion\":\"values\"})\n",
    "\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "random_float_array = np.random.uniform(7, 10.0, len(test_df.Emotion))\n",
    "test_df[\"emotion\"] = test_df['Emotion']\n",
    "test_df['Emotion'] = random_float_array.round(2)\n",
    "test_df = test_df.rename(columns={\"Emotion\":\"values\"})\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2Model\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "\n",
    "def getVggoud_proc(filename):\n",
    "    audio_length = 1.5\n",
    "    idx=0\n",
    "    samples, samplerate = librosa.load(filename)\n",
    "\n",
    "    if samples.shape[0] > 16000*(audio_length+0.1):\n",
    "        sample_indx = np.linspace(0, samples.shape[0]-16000*(audio_length+0.1), num=1, dtype=int)\n",
    "        samples = samples[sample_indx[idx]:sample_indx[idx]+int(16000*audio_length)]\n",
    "    else:\n",
    "        samples = np.tile(samples,int(audio_length))[:int(16000*audio_length)]\n",
    "\n",
    "    samples[samples > 1.] = 1\n",
    "    samples[samples < -1.] = -1\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def resample_audio(arr,sr):\n",
    "    return librosa.util.fix_length(arr, size=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_classification(df):\n",
    "    emotion_label = {'anger' : 1, 'disgust' : 2, 'fear' : 3,\n",
    "                 'joy' : 4, 'neutral' : 5, 'sadness' : 6, 'surprise' : 0}\n",
    "    labels = list(df['emotion'])\n",
    "    onlyfiles = list(df['filename'])\n",
    "    res = []\n",
    "    shapes = []\n",
    "    for a,label in zip(onlyfiles,labels):\n",
    "        try:\n",
    "            res1 = getVggoud_proc(a)\n",
    "            shapes.append(res1.shape)\n",
    "            res.append([res1,label])\n",
    "        except:\n",
    "            continue\n",
    "    shapes = np.array(shapes)\n",
    "    true_shapes = shapes>5000\n",
    "    res = [a for a,b in zip(res,true_shapes) if b[0] == True]\n",
    "    final_len = 16000\n",
    "    final_dataset = [[resample_audio(a[0],final_len),emotion_label[a[1]]] for a in res]\n",
    "    return final_dataset\n",
    "\n",
    "def get_dataset_regression(df):\n",
    "    labels = list(df['values'])\n",
    "    onlyfiles = list(df['filename'])\n",
    "    res = []\n",
    "    shapes = []\n",
    "    for a,label in zip(onlyfiles,labels):\n",
    "        try:\n",
    "            res1 = getVggoud_proc(a)\n",
    "            shapes.append(res1.shape)\n",
    "            res.append([res1,label])\n",
    "        except:\n",
    "            continue\n",
    "    shapes = np.array(shapes)\n",
    "    true_shapes = shapes>5000\n",
    "    res = [a for a,b in zip(res,true_shapes) if b[0] == True]\n",
    "    final_len = 16000\n",
    "    final_dataset = [[resample_audio(a[0],final_len),a[1]] for a in res]\n",
    "    return final_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      7.91\n",
       "1      7.45\n",
       "2      8.39\n",
       "3      8.06\n",
       "4      8.37\n",
       "       ... \n",
       "217    7.96\n",
       "218    9.39\n",
       "219    7.67\n",
       "220    8.39\n",
       "221    7.81\n",
       "Name: values, Length: 222, dtype: float64"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset_regression(train_df)\n",
    "test_dataset = get_dataset_regression(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2Model\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_dataset(dataset):\n",
    "    final_dataset = []\n",
    "    for a in dataset:\n",
    "        inputs = processor(a[0], sampling_rate=16000, return_tensors=\"pt\").to(device)\n",
    "        with t.no_grad():\n",
    "            outputs = model(**inputs).extract_features\n",
    "            final_dataset.append([outputs.squeeze(0),a[1]])\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset1 = get_final_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = get_final_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset1, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset1, batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(train_dataloader.dataset, 'wave2_vec_diff_reg_data_train_dataset.pth')\n",
    "\n",
    "# Save the testing DataLoader\n",
    "t.save(test_dataloader.dataset, 'wave2_vec_diff_reg_data_test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.set_device(8)\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionMLP_cls(nn.Module):\n",
    "    def softmax(self,x):\n",
    "        e_x = t.exp(x - t.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionMLP_cls, self).__init__()\n",
    "        self.input_matrix = nn.Parameter(t.rand(49,1))\n",
    "        self.fc1 = nn.Linear(512,64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = t.matmul(x.t(),self.input_matrix)\n",
    "        x = x.view(-1, 512)\n",
    "        x = t.relu(self.fc1(x))\n",
    "        x = t.relu(self.fc2(x))\n",
    "        x = t.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class EmotionMLP_reg(nn.Module):\n",
    "    def softmax(self,x):\n",
    "        e_x = t.exp(x - t.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EmotionMLP_reg, self).__init__()\n",
    "        self.input_matrix = nn.Parameter(t.rand(49,1))\n",
    "        self.fc1 = nn.Linear(512,32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = t.matmul(x.t(),self.input_matrix)\n",
    "        x = x.view(-1, 512)\n",
    "        x = t.relu(self.fc1(x))\n",
    "        x = t.relu(self.fc2(x)).double()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]], device='cuda:8', dtype=torch.float64, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmotionMLP_reg().to(device)\n",
    "\n",
    "x = t.rand(49,512)\n",
    "model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionMLP_reg().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(lossarr,epochs):\n",
    "    plt.plot(range(1,epochs+1),lossarr)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader,device):\n",
    "    \n",
    "    def accuracy1(y_true, y_pred):\n",
    "        eq = t.eq(y_true, y_pred).int()\n",
    "        return sum(eq)/len(eq)\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        # print(y_true,y_pred)\n",
    "        return t.mean(t.abs((y_true - y_pred) / y_true))*100\n",
    "    \n",
    "    acc = 0\n",
    "    mape = 0\n",
    "    with t.no_grad():\n",
    "        model.eval()\n",
    "        for x,y in test_loader:\n",
    "            x = x.squeeze(0)\n",
    "            outputs = model(x.to(device))\n",
    "            outputs1 = outputs.detach().cpu()\n",
    "            mape += mean_absolute_percentage_error(t.tensor([y]),outputs1)\n",
    "        print(f\"mape: {((mape/len(test_loader)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 4.437607669340425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:05<04:19,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 10.24863724485304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:24<03:34,  4.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Training Loss: 0.8703724133280127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:29<03:34,  4.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 9.644198253222918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:48<03:17,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Training Loss: 0.7131273722574898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [00:54<03:16,  5.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 10.82002799991086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [01:13<02:48,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Training Loss: 0.6388999929978475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [01:18<02:46,  4.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 9.816094045082696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [01:36<02:18,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Training Loss: 0.5866581090478398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 21/50 [01:41<02:19,  4.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 9.872178450134372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 25/50 [02:01<02:00,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Training Loss: 0.4893929232235058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [02:06<01:59,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 10.706542755195278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [02:24<01:31,  4.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Training Loss: 0.43763370189691714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 31/50 [02:29<01:31,  4.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 10.156423904555647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [02:49<01:12,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Training Loss: 0.38938840277615866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [02:54<01:10,  5.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 11.492060932939651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [03:13<00:47,  4.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Training Loss: 0.33791021347639005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [03:18<00:42,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 10.48106858193891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 45/50 [03:36<00:22,  4.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Training Loss: 0.3149215995444902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [03:41<00:19,  4.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 11.388329625794004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [04:00<00:00,  4.81s/it]\n"
     ]
    }
   ],
   "source": [
    "def train(model,train_loader,optimizer,criterion,num_epochs,device):\n",
    "    loss_arr = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # model.train()\n",
    "        total_loss = 0\n",
    "        for x,y in train_loader:\n",
    "            x = x.squeeze(0)\n",
    "            # Forward pass\n",
    "            outputs = model(x.to(device))\n",
    "            loss = criterion(outputs,t.tensor([y],dtype=float).to(device)).to(device)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        loss_arr.append(total_loss/len(train_loader))\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss/len(train_loader)}')\n",
    "            test(model,test_dataloader,device)\n",
    "    return loss_arr\n",
    "\n",
    "num_epochs = 50\n",
    "lossarr = train(model,train_dataloader,optimizer,criterion,num_epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAGwCAYAAACHJU4LAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAxBklEQVR4nO3de3SV9Z3v8c++71x3LkBITIAo5aqhFZUidryhVsXq1J7a1lll6vR47OCo6DjV09PRzpxpOO2yUxmpumY6UtdMi1Nd6KkcW1EhVgXlKogYRLkECYRLyD07O3v/zh87+wkpkOxssp9nJ3m/1nrWvj1JfnlWXHz8Pr/f9+cyxhgBAABkILfTAwAAADgTggoAAMhYBBUAAJCxCCoAACBjEVQAAEDGIqgAAICMRVABAAAZy+v0AM5GLBbTwYMHlZeXJ5fL5fRwAABAEowxamlpUVlZmdzu/msmwzqoHDx4UBUVFU4PAwAApKCurk7l5eX9njOsg0peXp6k+C+an5/v8GgAAEAympubVVFRYf073p9hHVQSt3vy8/MJKgAADDPJTNtgMi0AAMhYBBUAAJCxCCoAACBjEVQAAEDGIqgAAICMRVABAAAZi6ACAAAyFkEFAABkLIIKAADIWAQVAACQsQgqAAAgYxFUAABAxhrWmxKmS2ckqmNtXfK4XBofCjo9HAAARi0qKqfx/7bXa96SN/Tg8+87PRQAAEY1gsppBH0eSVI4EnN4JAAAjG4EldMI+uKXpbM76vBIAAAY3QgqpxHwxisqnRGCCgAATiKonIZVUeHWDwAAjiKonEaiohLm1g8AAI4iqJxGYjItFRUAAJxFUDmNgDdx64eKCgAATiKonIa1PLk7JmOMw6MBAGD0IqicRmIyrRQPKwAAwBkEldNIVFQkmr4BAOAkgsppeN0uuV3x5zR9AwDAOQSV03C5XCet/CGoAADgFILKGZw8oRYAADiDoHIGQZYoAwDgOILKGQRo+gYAgOMIKmdA0zcAAJxHUDkD5qgAAOA8gsoZ9O6gTEUFAACnEFTOILGDMkEFAADnEFTOIFFR4dYPAADOIaicAQ3fAABwHkHlDIJeJtMCAOA0gsoZBJhMCwCA4wgqZ8DyZAAAnEdQOQNa6AMA4DyCyhkEmEwLAIDjCCpn0NtCn1s/AAA4haByBr1zVKioAADgFILKGQTZPRkAAMcRVM6A3ZMBAHAeQeUMrIoKy5MBAHAMQeUMrL1+qKgAAOCYjAkqS5Yskcvl0n333ef0UCTR8A0AgEyQEUFlw4YNevrpp1VVVeX0UCzMUQEAwHmOB5XW1lbdfvvt+td//VcVFhb2e244HFZzc3OfI13YPRkAAOc5HlQWLVqkG2+8UfPnzx/w3OrqaoVCIeuoqKhI27jYPRkAAOc5GlRWrFihzZs3q7q6OqnzH374YTU1NVlHXV1d2sYWPGn3ZGNM2n4OAAA4M69TP7iurk733nuvVq9erWAwmNTXBAIBBQKBNI+s52f1VFRiRopEjfxely0/FwAA9HIsqGzatEkNDQ268MILrfei0ajefPNNPfHEEwqHw/J4PE4NTwFfb7Gpszsqv9fxu2QAAIw6jgWVq6++Wtu3b+/z3ne+8x1NmzZN3//+9x0NKVJ81Y/LJRkjhSMxKbmiDwAAGEKOBZW8vDydf/75fd7LyclRcXHxKe87weVyKeB1qzMSY+UPAAAO4X5GPwJedlAGAMBJjlVUTmft2rVOD6GPoM+tpg52UAYAwClUVPrR20afigoAAE4gqPQj0fSNigoAAM4gqPQj4GO/HwAAnERQ6QcVFQAAnEVQ6UeiosIcFQAAnEFQ6UfvDspUVAAAcAJBpR8BL3NUAABwEkGlH1ZFhVs/AAA4gqDSj2Bijgq3fgAAcARBpR/Wqh8qKgAAOIKg0o8AFRUAABxFUOlHbx8VKioAADiBoNKP3r1+qKgAAOAEgko/grTQBwDAUQSVfgS49QMAgKMIKv3obaHPrR8AAJxAUOlHbwt9KioAADiBoNKP3hb6VFQAAHACQaUftNAHAMBZBJV+WMuTqagAAOAIgko/rL1+qKgAAOAIgko/epcnU1EBAMAJBJV+0PANAABnEVT6kdjrpztm1B2lqgIAgN0IKv1ITKaVaPoGAIATCCr9SPRRkbj9AwCAEwgq/XC7XfJ7euapUFEBAMB2BJUBWPv9UFEBAMB2BJUB9O73Q0UFAAC7EVQGYO33Q9M3AABsR1AZADsoAwDgHILKAHrb6HPrBwAAuxFUBpBo+sZkWgAA7EdQGUDAaqNPRQUAALsRVAYQ9DJHBQAApxBUBpCYTMscFQAA7EdQGUCAHZQBAHAMQWUAAS8N3wAAcApBZQCJ5ck0fAMAwH4ElQFYc1SoqAAAYDuCygCsVT9UVAAAsB1BZQBMpgUAwDkElQEEezYl5NYPAAD2I6gMoLePChUVAADsRlAZQO/uyVRUAACwG0FlAAEvc1QAAHAKQWUAVkWFWz8AANiOoDKAxKofJtMCAGA/gsoAqKgAAOAcgsoAeueoUFEBAMBuBJUB9K76oaICAIDdCCoD6O2jQkUFAAC7EVQGkLj109UdUyxmHB4NAACjC0FlAImKikRVBQAAuxFUBpDY60eijT4AAHYjqAzA63HL63ZJYuUPAAB2I6gkgTb6AAA4g6CSBJq+AQDgDIJKEqwlytz6AQDAVgSVJCT2++HWDwAA9iKoJCHgTdz6oaICAICdCCpJCFJRAQDAEQSVJAS9tNEHAMAJBJUkUFEBAMAZBJUkJOaohAkqAADYiqCShN6KCrd+AACwE0ElCVYfFRq+AQBgK4JKEqzOtFRUAACwFUElCez1AwCAMwgqSQiw1w8AAI5wNKg8+eSTqqqqUn5+vvLz8zV37ly98sorTg7ptBKTadnrBwAAezkaVMrLy7VkyRJt2rRJGzdu1FVXXaWbb75ZO3bscHJYpwjSQh8AAEd4nfzhN910U5/X//RP/6Qnn3xS69ev18yZM085PxwOKxwOW6+bm5vTPkaJTQkBAHBKxsxRiUajWrFihdra2jR37tzTnlNdXa1QKGQdFRUVtozNqqgQVAAAsJXjQWX79u3Kzc1VIBDQXXfdpZUrV2rGjBmnPffhhx9WU1OTddTV1dkyxt4+Ktz6AQDATo7e+pGkqVOnauvWrWpqatLzzz+vhQsXqqam5rRhJRAIKBAI2D7G3sm0VFQAALCT40HF7/dr8uTJkqTZs2drw4YNevzxx/X00087PLJeAS8N3wAAcILjt37+VCwW6zNhNhNYe/3QRwUAAFs5WlF5+OGHdf3112vChAlqaWnRr3/9a61du1Z/+MMfnBzWKaw5KlRUAACwlaNBpaGhQd/+9rdVX1+vUCikqqoq/eEPf9A111zj5LBOQUUFAABnOBpUfvnLXzr545MWYHkyAACOyLg5Kpmot+FbTMYYh0cDAMDoQVBJQmKOiiR1RZmnAgCAXQgqSQh4ey8TS5QBALAPQSUJfo9bLlf8OU3fAACwD0ElCS6X66T9fqioAABgF4JKkqw2+ixRBgDANgSVJNFGHwAA+xFUkkTTNwAA7EdQSVJiiTJN3wAAsA9BJUkB9vsBAMB2BJUkJXqpcOsHAAD7EFSS1Hvrh4oKAAB2IagkKehleTIAAHYjqCSJigoAAPYjqCTJmqPCqh8AAGxDUElS0Fr1Q1ABAMAuBJUk9bbQ59YPAAB2IagkiYZvAADYj6CSpN45KlRUAACwC0ElSVZFheXJAADYhqCSJFroAwBgP4JKkoK00AcAwHYElSQFmEwLAIDtCCpJCjKZFgAA2xFUkmQ1fKOPCgAAtiGoJInOtAAA2I+gkiT2+gEAwH4ElST19lHh1g8AAHYhqCTJ2uuHigoAALYhqCSJigoAAPYjqCQpMUclGjOKRAkrAADYgaCSpERFRWJCLQAAdiGoJClRUZHopQIAgF0IKklyuVwsUQYAwGYElUEI0EYfAABbpRRUfvWrX2nVqlXW67/7u79TQUGBLr30Uu3bt2/IBpdpgmxMCACArVIKKj/+8Y+VlZUlSVq3bp2WLVumn/zkJxozZowWL148pAPMJOz3AwCAvbypfFFdXZ0mT54sSXrxxRd166236s4779S8efN0xRVXDOX4Mkri1g9N3wAAsEdKFZXc3FwdO3ZMkvTqq6/qmmuukSQFg0F1dHQM3egyTG/TN4IKAAB2SKmics011+i73/2uvvCFL2jXrl264YYbJEk7duzQpEmThnJ8GSXRRp/JtAAA2COlisqyZcs0d+5cHTlyRC+88IKKi4slSZs2bdI3v/nNIR1gJumdo0JFBQAAO6RUUSkoKNATTzxxyvs/+tGPznpAmYzlyQAA2Culisrvf/97vfXWW9brZcuW6fOf/7y+9a1vqbGxccgGl2kCLE8GAMBWKQWVBx98UM3NzZKk7du364EHHtANN9ygPXv26P777x/SAWaSoDcRVKioAABgh5Ru/ezZs0czZsyQJL3wwgtasGCBfvzjH2vz5s3WxNqRKDGZljkqAADYI6WKit/vV3t7uyTptdde07XXXitJKioqsiotI1GAigoAALZKqaJy2WWX6f7779e8efP03nvv6bnnnpMk7dq1S+Xl5UM6wEzSuzyZigoAAHZIqaLyxBNPyOv16vnnn9eTTz6pc845R5L0yiuv6Mtf/vKQDjCTsDwZAAB7pVRRmTBhgl5++eVT3v/nf/7nsx5QJrPmqHDrBwAAW6QUVCQpGo3qxRdf1M6dOyVJM2fO1Fe+8hV5PJ4hG1ymseaoUFEBAMAWKQWV3bt364YbbtBnn32mqVOnSpKqq6tVUVGhVatW6bzzzhvSQWYKWugDAGCvlOao3HPPPTrvvPNUV1enzZs3a/Pmzdq/f78qKyt1zz33DPUYMwZzVAAAsFdKFZWamhqtX79eRUVF1nvFxcVasmSJ5s2bN2SDyzQsTwYAwF4pVVQCgYBaWlpOeb+1tVV+v/+sB5WpAixPBgDAVikFlQULFujOO+/Uu+++K2OMjDFav3697rrrLn3lK18Z6jFmjN4W+gQVAADskFJQWbp0qc477zzNnTtXwWBQwWBQl156qSZPnqyf//znQzzEzNHbQp9bPwAA2CGlOSoFBQV66aWXtHv3bmt58vTp0zV58uQhHVymCfqYowIAgJ2SDioD7Yq8Zs0a6/nPfvaz1EeUwQLeRMM3bv0AAGCHpIPKli1bkjrP5XKlPJhMZ1VUWJ4MAIAtkg4qJ1dMRqtEUIlEjaIxI4975IYyAAAyQUqTaUerxGRaiaZvAADYgaAyCImGbxITagEAsANBZRA8bpd8nvjtHnqpAACQfgSVQUo0faOXCgAA6UdQGaSAj+60AADYhaAySIleKgQVAADSj6AySEFrY0Ju/QAAkG4ElUFK9FJheTIAAOlHUBmk3ls/VFQAAEg3gsogUVEBAMA+jgaV6upqXXzxxcrLy9O4ceN0yy23qLa21skhDSjIqh8AAGzjaFCpqanRokWLtH79eq1evVqRSETXXnut2tranBxWvxKTaemjAgBA+iW9KWE6/P73v+/zevny5Ro3bpw2bdqkP/uzP3NoVP1LtNGnogIAQPo5GlT+VFNTkySpqKjotJ+Hw2GFw2HrdXNzsy3jOhnLkwEAsE/GTKaNxWK67777NG/ePJ1//vmnPae6ulqhUMg6KioqbB4lFRUAAOyUMUFl0aJF+uCDD7RixYoznvPwww+rqanJOurq6mwcYVzvqh8qKgAApFtG3Pq5++679fLLL+vNN99UeXn5Gc8LBAIKBAI2juw0Y6CFPgAAtnE0qBhj9Dd/8zdauXKl1q5dq8rKSieHk5Te5clUVAAASDdHg8qiRYv061//Wi+99JLy8vJ06NAhSVIoFFJWVpaTQzsjazItDd8AAEg7R+eoPPnkk2pqatIVV1yh0tJS63juueecHFa/rDkqVFQAAEg7x2/9DDeJOSq00AcAIP0yZtXPcEELfQAA7ENQGSQavgEAYB+CyiAFveyeDACAXQgqgxSgogIAgG0IKoNEC30AAOxDUBkkJtMCAGAfgsogJSbTstcPAADpR1AZpIC3d1PC4dgHBgCA4YSgMkiJiopEVQUAgHQjqAxSYo6KxDwVAADSjaAySD6PWx63SxIVFQAA0o2gkoLEfj9UVAAASC+CSgp6lyhTUQEAIJ0IKikIsoMyAAC2IKikgIoKAAD2IKikwM8cFQAAbEFQSQFt9AEAsAdBJQW00QcAwB4ElRSwgzIAAPYgqKQgUVHppKICAEBaEVRSkJijEqaiAgBAWhFUUhA8aQdlAACQPgSVFAR8LE8GAMAOBJUUsDwZAAB7EFRSELQavnHrBwCAdCKopCCQmEzLXj8AAKQVQSUFASoqAADYgqCSAuaoAABgD4JKCqygwvJkAADSiqCSAmuvHyoqAACkFUElBdZeP1RUAABIK4JKCqioAABgD4JKCphMCwCAPQgqKWCvHwAA7EFQSQF7/QAAYA+CSgoSFRUavgEAkF4ElRQkJtN2dkdljHF4NAAAjFwElRQk9voxRopECSoAAKQLQSUFib1+pHhVBQAApAdBJQUBr1suV/w5E2oBAEgfgkoKXC6XVVUJM6EWAIC0IaikKNH0LcytHwAA0oagkqJERYUlygAApA9BJUW00QcAIP0IKimi6RsAAOlHUEmRtYMyc1QAAEgbgkqKAlRUAABIO4JKitiYEACA9COopMiaTMutHwAA0oagkiKrjwq3fgAASBuCSoqsPipUVAAASBuCSoqCPhq+AQCQbgSVFCX6qLA8GQCA9CGopCix6oc5KgAApA9BJUW9nWmpqAAAkC4ElRSx1w8AAOlHUElRbwt9bv0AAJAuBJUUBbj1AwBA2hFUUhRgeTIAAGlHUEkRLfQBAEg/gkqKaKEPAED6EVRSRAt9AADSj6CSIioqAACkH0ElRYnlye1d3YrFjMOjAQBgZCKopGhMbkBet0uN7RH91a826Fhr2OkhAQAw4hBUUjQmN6Af//kFCnjdWlN7RNc//ke988lRp4cFAMCIQlA5C1+/uEIv3T1Pk8flqqElrNv/7V39bPUudUeZtwIAwFAgqJylaePz9X/vnqfbLqqQMdLS1z/Wt/71XdU3dTg9NAAAhj2CyhDI9nv1f75Wpce/8XnlBrx6b+9xXf/4H/Xah4edHhoAAMMaQWUI3fz5c/Ty31ymC84J6UR7RN99dqN+9LsdCtNrBQCAlDgaVN58803ddNNNKisrk8vl0osvvujkcIbEpDE5euF7l+q7l1VKkp55e6+++ot3tPdom8MjAwBg+HE0qLS1tWnWrFlatmyZk8MYcn6vW/9rwQz9+19epMJsn3YcbNaCf3lLq7bVOz00AACGFZcxJiO6lblcLq1cuVK33HJL0l/T3NysUCikpqYm5efnp29wZ6G+qUP3/GaLNuxtlCR9e+5E/eDG6Qp4PQ6PDAAAZwzm3+9hNUclHA6rubm5z5HpSkNZ+s1//6K+d8V5kqRn1+3T155cp33HuBUEAMBAhlVQqa6uVigUso6Kigqnh5QUr8et7395mp75zsUqzPZp+2dNWrD0Lb2ynVtBAAD0Z1gFlYcfflhNTU3WUVdX5/SQBuXKqeO06p4vafbEQrWEu/W9/9ysR/8vq4IAADiTYRVUAoGA8vPz+xzDTVlBllbc+UX9j8vPlSQtf2ev/ttT61R3vN3hkQEAkHmGVVAZKXwetx6+frp+ufAiFWT7tO1Ak25Y+kf9+t397MQMAMBJHA0qra2t2rp1q7Zu3SpJ2rNnj7Zu3ar9+/c7OSzbXD29RKvu+ZIunFCgls5u/c+V2/W1p97Rhwczf5IwAAB2cHR58tq1a3XllVee8v7ChQu1fPnyAb9+OCxPTkZ3NKZfrdunn71aq7auqDxul75z6STdd80U5Qa8Tg8PAIAhNZh/vzOmj0oqRkpQSTjU1Kl/eHmH/t/2Q5Kk0lBQj9w0Q9fNHC+Xy+Xw6AAAGBojto/KSDc+FNQvbp+tZ75zsSqKslTf1Km7/mOz7li+gcm2AIBRiYpKhuqMRLVszW49VfOJIlGjgNetRVdO1kWTChX0eZTVcySeB/1u+T1uKi8AgIzHrZ8RZHdDq3744gda9+mxAc91uaRsn0eXVBbp9jkTdeW0cfK4CS4AgMxCUBlhjDF6cetn+s/1+9XUEVFnd1QdXTF1RqLqiEQVPcOS5rJQULddPEG3XVyh8aGgzaMGAOD0CCqjTCQaU0ckqs5IVEdbuvTi1s/02411amyPSJI8bpfmTx+n2+dM1GWTx8hNlQUA4CCCCtQZieoPOw7pP9fv13t7j1vvTyjK1jcvmaD508epJBRUXsDLvBYAgK0IKuhj1+EW/frd/Xph8wG1dHb3+Szb71FJflDj8gIqyQ+qJD/xGNS5Y3M0ozSfIAMAGFIEFZxWR1dUv9t2UP+1oU67Dreo+U9Cy+lMLM7WjReUakFVmaaX5hFaAABnjaCCpHR0RXW4uTN+tITV0NypQ03x54ebOrXtsxPqjMSs888dm6MFF5RqwawyTSnJc3DkAIDhjKCCIdEW7tbrHzVo1baDWlN7RF3dvaHlc+NytaCqTNfOLFHlmBwFfR4HRwoAGE4IKhhyLZ0RvbbzsFZtq1fNriOKRPv+2RRm+1QaylJpKKjSgqBKQ1kanx9/Xl6QrYqiLG4bAQAkEVSQZk0dEb2645Be3lavDXuPq70rOuDXzCzL18JLJ+krs8qovgDAKEdQgW2MMWru6FZ9c4fqT3SqvqlT9U0dfR7rjrdbFZiiHL++cXGF/uKLE1VWkOXw6AEATiCoIKM0tnVpxYY6/cf6ffrsRIekeBO662aW6C8vrdTFkwq5LQQAowhBBRmpOxrTazsPa/k7e7X+094mdNNL8/WXl05URWG2Gtsjamzv0on2rpOe9z4W5/g159wizaks1uyJhcoJeB38jQAAqSCoIOPtrG/Ws+v2auWWz/osgR4Mr9ulC8pD+uK5xfriucW6iOACAMMCQQXDxon2Lj23oU4rt3ym7phRYbZPBdl+FWX7VZDjU2G233ovlOXT/uPtWv/pMb376XHrNlKCx+3SBeeE9PmKApUXZumcgiyVFWSptCCoMTkB9jgCgAxBUMGoUJcILXuOa/2nx3SgseOM5/o9bpUWBFUWioeXcfkBBbxu+b1u+T1u+Tzx5z6PWz6PSwGvW0GfRxdOLFR+0GfjbwUAIx9BBaPSgcZ2vfvpcdUebtHBEx09R6caWjoVS/Gv3O916/IpY7WgqlTzp5dwawkAhgBBBThJJBrT4eZOHTzRqYMnOvTZiQ4daQkrEo2pqzumSDSmSNSoq8/rmBpawtp3rN36PgGvW1dNG6cFVWW6ato4ZfnpBwMAqSCoAEPAGKPawy16+f16vbztoPaeFFqyfB5dPX2cFlSVakxuQC2d3WrujKi5I6Jm63nve0U5fl04oVCzJxZq2vg8eT1uB38zAHAWQQUYYsYY7TjYrJe3xUNLf/NhBpLl82hWRcgKLl+YUKiiHP8QjhYAMhtBBUgjY4y2HWjSy9sO6vWPGhSNGeUHfcrP8iovEH/MD/qU1/NebsCrgyc6tWl/o7bsb1RLZ/cp3/PcMTm6aFKhrpo2Tpd9bqxymQsDYAQjqAAZKhYz2n2kVZv3NWrTvkZt3t+oT4609TnH53FpTmWxrpo2TldPH6eJxTkOjRYA0oOgAgwjJ9q7tHl/o976+Jje+Ohwn7kwknTe2BxdNW2crppWoosmFcp30vwWY4xiRorGjGImcUhul+R2ueRxu+RxueghAyCjEFSAYezTI61646MGvb6zQRv2Hlf3SWurvW6X3C6XYsYoaowG819vb2iRfG63/mzKWH3/y9M0oTg7Db8FAJwZQQUYIZo7I/rjrqN6/aPDWlt7RMfbuob0+/s9bn1n3iQtumoyje0A2IagAoxA0ZjRoeZOuRSvjrhcildIem7tuF0976u34hKLmZ7bQoq/1/P6SGtY/7x6l/748VFJUnGOX/dfO0W3XVTB0mkAaUdQATAgY4zW1Dbof6/aqU97JvROG5+n/3XjDF32uTEOjw7ASEZQAZC0SDSm/1i/Tz9/7WM1dUQkSVdPG6f/eeN0nTc2V8YYnWiPaN/xdu0/3q664+3ad6yt53mHmjsiCvo9yvZ7lOWLP2b7vco66XUoy6cZZfmaVV6gicXZcrmY3AuMZgQVAIN2or1LP3/tY/3H+n3qjhl53S5NHperzxo71BI+tfdLqkJZPlWVhzSrvCD+WFGgkvzgkH1/AJmPoAIgZZ8cadWPV+3U6x819Hm/JD+gCUXZqijK1sSiHE0oztKEomwVZvvVEYmqoyuq9p6jI9Ktjq6Y2ru61dEV1ZHWsLYdaNKHB5vVFY2d8jPH5wdVVR7S9NJ8TR2fpykleZpUnM18GWCEIqgAOGvv153QkZawJhZnq7wwe0g2Yezqjqn2UIveP3BC2w6c0Pt1Tfq4oeW0u1v7vW5NHptrBZep43M1pSRPuQGvunsmBXfHjKLR+MThaCym7phRd9Qo2+/R+FBQ2X46/AKZiKACYNhoC3drx8FmbTtwQrsOt6j2UIt2HW5VRyR61t87P+jV+FBQJflBjc8PqjQUVEko/rw4NyC/xy2/162AN/6YeO33uuV1u5hLA6QJQQXAsBaLGR1o7FDt4RbVHmpW7eFW7TrUok+OtFoN8LzueOddrzu+PDv+2i2PW2rt7FZb19kFHZdLyvZ5VF6YrQnF2ZpQlK2JPY8TiuJVJr+XW1NAKggqAEakaE9Icbs0YLWjpTOiQ02dOtTcGX/seX64uVP1TZ060R5RuDumru6ouqIxdXXHTnsL6kzcLqk0lKWJxdmaXpqvqvKQzj8npMriHLYsAAYwmH+/uYELYNjwDCIA5PXsYP25krykv6Y7GrNCS1d3TC3hbtX1LMvef6xd+6zl2e3qiET12YkOfXaiQ+98csz6HrkBr84/J19V5QU6/5yQqs4JsSQbOAsEFQDo4fW45fW4le2Pvx4n6byxuaecZ4zR0dYu7T/epk+PtFlzbHYcbFZruFvrPz2u9Z8et87PC3o1NjcgX88cGJ/HZT33e9zyedzyed0Ket1W/5mgz2M9z/J5FOx5npg/4zn5cPU+97pd8nvdGpsXYDIxRgT+igFgkFwul8bmBTQ2L6DZE4v033re747GtPtIq7YdaNL2A03a/lmTPqxvVktnt1o6h64XTbLygl6V5AdVkh/oeQyqJC/+fHwoqGnj84dkNReQTsxRAYA0ikRj2t3QqpbObnV1xxTpub0U6Tm6umPqihrrdlNHJKrOnr40HZH40Xny80gsvofTSXs3xZdqxxSNSdFYTJ2RWFKrpnwel2aVF+iSyiLNObdYsycWKjfA/78i/ZhMCwCjmDFGreFuHW4Oq6G5U4dbOnWoKazDzZ1qaOnU4eaw9h9v15GWcJ+v87hdOv+ckOZUFmlOZZEumlSkUBa7amPoEVQAAP0yxqjueIfW7zmmdz89rnf3HNOBxo4+57hc0pjcgIqy/SrI9qkox6/CHL8Ks30qzParMNuvohy/jIyOt0V0or1Lx9u61Gg9RtTY81pyaXppnmaU5mtGWb5mlOarckwO3YdHKYIKAGDQPjvRofes4HJce462pfXnBbxuTRufZwWX88bmSi6pOxq/lRWJGut5V3e887AxUlV5SDNK81kGPowRVAAAZ+1oa/x2UWNbRMfbu3orJolqSc9rSSrKiVdXeistvp7qS/zoisb0UX2zPqxv1ocHm7WzvvmsmvKNyQ3oiqljdcXUsfrS5LEKZXOLajghqAAAMlosZrT/eLsVXD6sb9beY23yuFzyeuJLuL3uk5/HH8PdMW3a16j2k0KOx+3SFyoKdOW0cbp8yljNLMs/bd8aY4wiUaOOSFThSLzRX6Jik9gnKhKNKRqLn9cdiyng9ejcsTkqzvHTC2cIEVQAACNWuDuqjXsbtba2QWtqj2h3Q2ufz8fmBTQ2N6DO7viKqc7umDp7VlMNpvvwyQqzfZo8LrfnyLOel4WCBJgUEFQAAKNG3fF21ew6orW1DXp797Gklma7XLKa7Xl7KjbxCk68GV+ieV5bV7cONHboTP9S5vg9Om9criYV52jSmBxVjsnWpOIcVY7JUUGicyBOQVABAIxK4e6otu4/oY5I1OrwGz/cyvJ5FOh57ve4k66EdHRF9enRVu1u6D0+bmjV3qNt1iaZp1OY7YuHl+IcVRRlK2aM2sJRtYW71drVrfZwd/x1V7fawvGNNHMDXpXkBzQ+0aCvpzlfonHfuLzgWW+GaYyxVmXlZ3lVmO2Xz+bVVwQVAADSLBKNad+xdu1uaNXeY23ae7RNe462ae+xNh1uDg/8DVJUlOPX2NyAxuT5NSY3YB1j8wIakxt/z+91q76pUwdPdKj+RIcOJp73PIa7Y32+Z14wHlgKc/wqyvb1PMZfzyjN15XTxg3p78CmhAAApJnP47bmqvyp9q5u7T3arr3H4uHlQGOHfB6XcgJe5fg9PY/e+GMg/jrL5+lp1Ne723dDc9jaAbyhpVORaLwacrytS7WHz278eUGvWsPdMkbWNg/7j7efct6CqtIhDyqDQVABAGCIZfu98f4wZUNX7Y/FjI63d+lIS1hHW+NH/HmXjraEdaQ1/vxIS1hd3VGVFWSpNBRUaUGWzkk8D8Wfl4QCCng9isaMmjtOXn4eb9J3vL3LatY3q6JgyH6HVBBUAAAYBtxul3WbZ6h43K54v5uczJ34S+9iAACQsQgqAAAgYxFUAABAxiKoAACAjEVQAQAAGYugAgAAMhZBBQAAZCyCCgAAyFgEFQAAkLEIKgAAIGMRVAAAQMYiqAAAgIxFUAEAABmLoAIAADKW1+kBnA1jjCSpubnZ4ZEAAIBkJf7dTvw73p9hHVRaWlokSRUVFQ6PBAAADFZLS4tCoVC/57hMMnEmQ8ViMR08eFB5eXlyuVxJfU1zc7MqKipUV1en/Pz8NI8QEtfcblxve3G97cX1tle6rrcxRi0tLSorK5Pb3f8slGFdUXG73SovL0/pa/Pz8/kjtxnX3F5cb3txve3F9bZXOq73QJWUBCbTAgCAjEVQAQAAGWvUBZVAIKBHHnlEgUDA6aGMGlxze3G97cX1thfX216ZcL2H9WRaAAAwso26igoAABg+CCoAACBjEVQAAEDGIqgAAICMNeqCyrJlyzRp0iQFg0HNmTNH7733ntNDGhHefPNN3XTTTSorK5PL5dKLL77Y53NjjP7+7/9epaWlysrK0vz58/Xxxx87M9gRoLq6WhdffLHy8vI0btw43XLLLaqtre1zTmdnpxYtWqTi4mLl5ubq1ltv1eHDhx0a8fD25JNPqqqqymp6NXfuXL3yyivW51zr9FqyZIlcLpfuu+8+6z2u+dB59NFH5XK5+hzTpk2zPnf6Wo+qoPLcc8/p/vvv1yOPPKLNmzdr1qxZuu6669TQ0OD00Ia9trY2zZo1S8uWLTvt5z/5yU+0dOlSPfXUU3r33XeVk5Oj6667Tp2dnTaPdGSoqanRokWLtH79eq1evVqRSETXXnut2trarHMWL16s3/3ud/rtb3+rmpoaHTx4UF/96lcdHPXwVV5eriVLlmjTpk3auHGjrrrqKt18883asWOHJK51Om3YsEFPP/20qqqq+rzPNR9aM2fOVH19vXW89dZb1meOX2szilxyySVm0aJF1utoNGrKyspMdXW1g6MaeSSZlStXWq9jsZgZP368+elPf2q9d+LECRMIBMxvfvMbB0Y48jQ0NBhJpqamxhgTv74+n8/89re/tc7ZuXOnkWTWrVvn1DBHlMLCQvNv//ZvXOs0amlpMZ/73OfM6tWrzeWXX27uvfdeYwx/30PtkUceMbNmzTrtZ5lwrUdNRaWrq0ubNm3S/Pnzrffcbrfmz5+vdevWOTiykW/Pnj06dOhQn2sfCoU0Z84crv0QaWpqkiQVFRVJkjZt2qRIJNLnmk+bNk0TJkzgmp+laDSqFStWqK2tTXPnzuVap9GiRYt044039rm2En/f6fDxxx+rrKxM5557rm6//Xbt379fUmZc62G9KeFgHD16VNFoVCUlJX3eLykp0UcffeTQqEaHQ4cOSdJpr33iM6QuFovpvvvu07x583T++edLil9zv9+vgoKCPudyzVO3fft2zZ07V52dncrNzdXKlSs1Y8YMbd26lWudBitWrNDmzZu1YcOGUz7j73tozZkzR8uXL9fUqVNVX1+vH/3oR/rSl76kDz74ICOu9agJKsBItWjRIn3wwQd97ilj6E2dOlVbt25VU1OTnn/+eS1cuFA1NTVOD2tEqqur07333qvVq1crGAw6PZwR7/rrr7eeV1VVac6cOZo4caL+67/+S1lZWQ6OLG7U3PoZM2aMPB7PKTOVDx8+rPHjxzs0qtEhcX259kPv7rvv1ssvv6w1a9aovLzcen/8+PHq6urSiRMn+pzPNU+d3+/X5MmTNXv2bFVXV2vWrFl6/PHHudZpsGnTJjU0NOjCCy+U1+uV1+tVTU2Nli5dKq/Xq5KSEq55GhUUFGjKlCnavXt3Rvx9j5qg4vf7NXv2bL3++uvWe7FYTK+//rrmzp3r4MhGvsrKSo0fP77PtW9ubta7777LtU+RMUZ33323Vq5cqTfeeEOVlZV9Pp89e7Z8Pl+fa15bW6v9+/dzzYdILBZTOBzmWqfB1Vdfre3bt2vr1q3WcdFFF+n222+3nnPN06e1tVWffPKJSktLM+Pv25YpuxlixYoVJhAImOXLl5sPP/zQ3HnnnaagoMAcOnTI6aENey0tLWbLli1my5YtRpL52c9+ZrZs2WL27dtnjDFmyZIlpqCgwLz00ktm27Zt5uabbzaVlZWmo6PD4ZEPT9/73vdMKBQya9euNfX19dbR3t5unXPXXXeZCRMmmDfeeMNs3LjRzJ0718ydO9fBUQ9fDz30kKmpqTF79uwx27ZtMw899JBxuVzm1VdfNcZwre1w8qofY7jmQ+mBBx4wa9euNXv27DFvv/22mT9/vhkzZoxpaGgwxjh/rUdVUDHGmH/5l38xEyZMMH6/31xyySVm/fr1Tg9pRFizZo2RdMqxcOFCY0x8ifIPf/hDU1JSYgKBgLn66qtNbW2ts4Mexk53rSWZZ555xjqno6PD/PVf/7UpLCw02dnZ5s///M9NfX29c4Mexu644w4zceJE4/f7zdixY83VV19thRRjuNZ2+NOgwjUfOrfddpspLS01fr/fnHPOOea2224zu3fvtj53+lq7jDHGntoNAADA4IyaOSoAAGD4IagAAICMRVABAAAZi6ACAAAyFkEFAABkLIIKAADIWAQVAACQsQgqAAAgYxFUAAxra9eulcvlOmXTNAAjA0EFAABkLIIKAADIWAQVAGclFoupurpalZWVysrK0qxZs/T8889L6r0ts2rVKlVVVSkYDOqLX/yiPvjggz7f44UXXtDMmTMVCAQ0adIkPfbYY30+D4fD+v73v6+KigoFAgFNnjxZv/zlL/ucs2nTJl100UXKzs7WpZdeqtraWuuz999/X1deeaXy8vKUn5+v2bNna+PGjWm6IgCGEkEFwFmprq7Ws88+q6eeeko7duzQ4sWL9Rd/8ReqqamxznnwwQf12GOPacOGDRo7dqxuuukmRSIRSfGA8fWvf13f+MY3tH37dj366KP64Q9/qOXLl1tf/+1vf1u/+c1vtHTpUu3cuVNPP/20cnNz+4zjBz/4gR577DFt3LhRXq9Xd9xxh/XZ7bffrvLycm3YsEGbNm3SQw89JJ/Pl94LA2Bo2LZPM4ARp7Oz02RnZ5t33nmnz/t/9Vd/Zb75zW+aNWvWGElmxYoV1mfHjh0zWVlZ5rnnnjPGGPOtb33LXHPNNX2+/sEHHzQzZswwxhhTW1trJJnVq1efdgyJn/Haa69Z761atcpIMh0dHcYYY/Ly8szy5cvP/hcGYDsqKgBStnv3brW3t+uaa65Rbm6udTz77LP65JNPrPPmzp1rPS8qKtLUqVO1c+dOSdLOnTs1b968Pt933rx5+vjjjxWNRrV161Z5PB5dfvnl/Y6lqqrKel5aWipJamhokCTdf//9+u53v6v58+dryZIlfcYGILMRVACkrLW1VZK0atUqbd261To+/PBDa57K2crKykrqvJNv5bhcLknx+TOS9Oijj2rHjh268cYb9cYbb2jGjBlauXLlkIwPQHoRVACkbMaMGQoEAtq/f78mT57c56ioqLDOW79+vfW8sbFRu3bt0vTp0yVJ06dP19tvv93n+7799tuaMmWKPB6PLrjgAsVisT5zXlIxZcoULV68WK+++qq++tWv6plnnjmr7wfAHl6nBwBg+MrLy9Pf/u3favHixYrFYrrsssvU1NSkt99+W/n5+Zo4caIk6R/+4R9UXFyskpIS/eAHP9CYMWN0yy23SJIeeOABXXzxxfrHf/xH3XbbbVq3bp2eeOIJ/eIXv5AkTZo0SQsXLtQdd9yhpUuXatasWdq3b58aGhr09a9/fcAxdnR06MEHH9TXvvY1VVZW6sCBA9qwYYNuvfXWtF0XAEPI6UkyAIa3WCxmfv7zn5upU6can89nxo4da6677jpTU1NjTXT93e9+Z2bOnGn8fr+55JJLzPvvv9/nezz//PNmxowZxufzmQkTJpif/vSnfT7v6OgwixcvNqWlpcbv95vJkyebf//3fzfG9E6mbWxstM7fsmWLkWT27NljwuGw+cY3vmEqKiqM3+83ZWVl5u6777Ym2gLIbC5jjHE4KwEYodauXasrr7xSjY2NKigocHo4AIYh5qgAAICMRVABAAAZi1s/AAAgY1FRAQAAGYugAgAAMhZBBQAAZCyCCgAAyFgEFQAAkLEIKgAAIGMRVAAAQMYiqAAAgIz1/wE64YaNNAf4oAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(lossarr,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def test(model,test_loader,device):\n",
    "    \n",
    "    def accuracy1(y_true, y_pred):\n",
    "        eq = t.eq(y_true, y_pred).int()\n",
    "        return sum(eq)/len(eq)\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        # print(y_true,y_pred)\n",
    "        return t.mean(t.abs((y_true - y_pred) / y_true))*100\n",
    "    \n",
    "    def MAE(y_true,y_pred):\n",
    "        y_true = y_true.detach().numpy()\n",
    "        y_pred = y_pred.detach().numpy()\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "    \n",
    "    acc = 0\n",
    "    mape = 0\n",
    "    mae = 0\n",
    "    with t.no_grad():\n",
    "        model.eval()\n",
    "        for x,y in test_loader:\n",
    "            x = x.squeeze(0)\n",
    "            outputs = model(x.to(device))\n",
    "            outputs1 = outputs.detach().cpu()\n",
    "            mape += mean_absolute_percentage_error(t.tensor([y]),outputs1)\n",
    "            mae += MAE(t.tensor([y]),outputs1)\n",
    "        print(f\"mape: {((mape/len(test_loader))) :0.2f}\")\n",
    "        print(f\"mae: {((mae/len(test_loader))) :0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 10.73\n",
      "mae: 0.88\n"
     ]
    }
   ],
   "source": [
    "test(model,test_dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
