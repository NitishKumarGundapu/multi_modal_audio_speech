{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8da46410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 405\n",
      "Number of testing batches: 102\n",
      "({'modality': 3, 'vocal_channel': 2, 'emotion': 4, 'intensity': 1, 'statement': 2, 'repetition': 2, 'gender': 1}, {'modality': 3, 'vocal_channel': 2, 'emotion': 6, 'intensity': 2, 'statement': 1, 'repetition': 1, 'gender': 1})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "from transformers import AutoProcessor\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_samples, target_length=16000*3):  # 3 seconds at 16kHz\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_samples (list): List of tuples (audio_path, emotion_label)\n",
    "            target_length (int): Target length in samples (default 3 seconds)\n",
    "        \"\"\"\n",
    "        self.data_samples = data_samples\n",
    "        self.audio_processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, emotion_label = self.data_samples[idx]\n",
    "        \n",
    "        # Extract metadata from filename\n",
    "        audio_file = os.path.basename(audio_path)\n",
    "        parts = audio_file.split(\"-\")\n",
    "        \n",
    "        # Parse all metadata\n",
    "        modality = int(parts[0])\n",
    "        vocal_channel = int(parts[1])\n",
    "        emotion = int(parts[2])\n",
    "        intensity = int(parts[3])\n",
    "        statement = int(parts[4])\n",
    "        repetition = int(parts[5])\n",
    "        actor = int(parts[6].split(\".\")[0])\n",
    "        gender = 0 if actor % 2 == 1 else 1\n",
    "\n",
    "        # Load and process audio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path, format=\"wav\")\n",
    "        \n",
    "        # Resample to 16kHz if needed\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Pad or truncate to target length\n",
    "        if waveform.shape[1] < self.target_length:\n",
    "            # Pad with zeros\n",
    "            pad_amount = self.target_length - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
    "        elif waveform.shape[1] > self.target_length:\n",
    "            # Random crop\n",
    "            start = torch.randint(0, waveform.shape[1] - self.target_length, (1,)).item()\n",
    "            waveform = waveform[:, start:start+self.target_length]\n",
    "        \n",
    "        # Process with wav2vec processor\n",
    "        audio_input = self.audio_processor(\n",
    "            waveform.squeeze(0), \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        label = {\n",
    "            \"modality\": modality,\n",
    "            \"vocal_channel\": vocal_channel,\n",
    "            \"emotion\": emotion,\n",
    "            \"intensity\": intensity,\n",
    "            \"statement\": statement,\n",
    "            \"repetition\": repetition,\n",
    "            \"gender\": gender,\n",
    "        }\n",
    "\n",
    "        return audio_input, label['emotion'], label\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle wav2vec2 processor outputs.\n",
    "    Pads the audio inputs to the longest in the batch.\n",
    "    \"\"\"\n",
    "    audio_inputs, emotions, labels = zip(*batch)\n",
    "    \n",
    "    # Get max length in this batch\n",
    "    max_length = max([item.input_values.shape[1] for item in audio_inputs])\n",
    "    \n",
    "    # Pad all inputs to max length\n",
    "    padded_inputs = []\n",
    "    for item in audio_inputs:\n",
    "        pad_amount = max_length - item.input_values.shape[1]\n",
    "        padded = torch.nn.functional.pad(\n",
    "            item.input_values, \n",
    "            (0, pad_amount), \n",
    "            value=0\n",
    "        )\n",
    "        padded_inputs.append(padded)\n",
    "    \n",
    "    # Stack all padded tensors\n",
    "    audio_input_values = torch.stack(padded_inputs).squeeze(1)\n",
    "    \n",
    "    # Stack attention masks if they exist\n",
    "    if hasattr(audio_inputs[0], 'attention_mask'):\n",
    "        padded_masks = []\n",
    "        for item in audio_inputs:\n",
    "            pad_amount = max_length - item.attention_mask.shape[1]\n",
    "            padded = torch.nn.functional.pad(\n",
    "                item.attention_mask,\n",
    "                (0, pad_amount),\n",
    "                value=0\n",
    "            )\n",
    "            padded_masks.append(padded)\n",
    "        attention_mask = torch.stack(padded_masks).squeeze(1)\n",
    "    else:\n",
    "        attention_mask = None\n",
    "    \n",
    "    # Stack other elements\n",
    "    emotions = torch.tensor(emotions)\n",
    "    \n",
    "    return {\n",
    "        'input_values': audio_input_values,\n",
    "        'attention_mask': attention_mask,\n",
    "        'emotions': emotions,\n",
    "        'metadata': labels\n",
    "    }\n",
    "\n",
    "\n",
    "def load_ravdess_dataset(base_dir, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load the RAVDESS dataset and split it into training and testing datasets.\n",
    "\n",
    "    Args:\n",
    "        base_dir (str): Path to the RAVDESS dataset containing actor folders.\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset (AudioDataset): Training dataset\n",
    "        test_dataset (AudioDataset): Testing dataset\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for actor_folder in os.listdir(base_dir):\n",
    "        actor_path = os.path.join(base_dir, actor_folder)\n",
    "        if os.path.isdir(actor_path):\n",
    "            for audio_file in os.listdir(actor_path):\n",
    "                if audio_file.endswith(\".wav\"):\n",
    "                    parts = audio_file.split(\"-\")\n",
    "                    emotion = int(parts[2])\n",
    "                    audio_path = os.path.join(actor_path, audio_file)\n",
    "                    data.append((audio_path, emotion))\n",
    "\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    train_dataset = AudioDataset(train_data)\n",
    "    test_dataset = AudioDataset(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_data_loaders(base_dir, batch_size=8, test_size=0.2, random_state=42):\n",
    "    train_dataset, test_dataset = load_ravdess_dataset(base_dir, test_size, random_state)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn  # Use our custom collate function\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = r\"C:\\Users\\gnith\\Desktop\\multi_modal_audio_speech\\ravdess_dataset\"\n",
    "    \n",
    "    # Get data loaders\n",
    "    train_loader, test_loader = get_data_loaders(base_dir, batch_size=2)\n",
    "    \n",
    "    print(f\"Number of training batches: {len(train_loader)}\")\n",
    "    print(f\"Number of testing batches: {len(test_loader)}\")\n",
    "    \n",
    "    # Example: Get a batch from the training loader\n",
    "    for batch in train_loader:\n",
    "        print(batch['metadata'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8adbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnith\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import Wav2Vec2Model\n",
    "from audio_dataset import get_data_loaders\n",
    "\n",
    "audio_encoder = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f928bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_dataset import get_data_loaders\n",
    "\n",
    "audio_dir = r\"C:\\Users\\gnith\\Desktop\\multi_modal_audio_speech\\ravdess_dataset\"\n",
    "\n",
    "    # Get train and validation dataloaders\n",
    "train_loader, val_loader = get_data_loaders(audio_dir, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1706f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_values': tensor([[-7.0262e-03, -8.2524e-03, -1.0697e-02,  ..., -7.0473e-03,\n",
      "         -1.2591e-01, -2.1741e-01],\n",
      "        [-3.1370e-06, -1.3217e-05, -9.6774e-06,  ..., -1.0263e-05,\n",
      "         -1.1941e-05, -2.2046e-06]]), 'attention_mask': None, 'emotions': tensor([2, 6]), 'metadata': ({'modality': 3, 'vocal_channel': 2, 'emotion': 2, 'intensity': 2, 'statement': 1, 'repetition': 1, 'gender': 0}, {'modality': 3, 'vocal_channel': 1, 'emotion': 6, 'intensity': 2, 'statement': 2, 'repetition': 2, 'gender': 1})}\n"
     ]
    }
   ],
   "source": [
    "for a in train_loader:\n",
    "    print(a)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b049285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 149, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_encoder(a['input_values']).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c4d9e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_encoder(a['input_values']).extract_features.mean(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from transformers import AutoProcessor\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_samples, target_length=16000*3, augment=False, n_mels=128, n_fft=1024, hop_length=512):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_samples (list): List of tuples (audio_path, emotion_label)\n",
    "            target_length (int): Target length in samples (default 3 seconds)\n",
    "            augment (bool): Whether to apply audio augmentation\n",
    "            n_mels (int): Number of Mel bins\n",
    "            n_fft (int): FFT window size\n",
    "            hop_length (int): Hop length between STFT windows\n",
    "        \"\"\"\n",
    "        self.data_samples = data_samples\n",
    "        self.target_length = target_length\n",
    "        self.augment = augment\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        \n",
    "        # Spectrogram transforms\n",
    "        self.mel_spectrogram = T.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_mels=n_mels,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            power=2.0  # Use power spectrum (magnitude squared)\n",
    "        )\n",
    "        \n",
    "        # For log compression\n",
    "        self.amplitude_to_db = T.AmplitudeToDB()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path, emotion_label = self.data_samples[idx]\n",
    "        \n",
    "        # Extract metadata from filename\n",
    "        audio_file = os.path.basename(audio_path)\n",
    "        parts = audio_file.split(\"-\")\n",
    "        \n",
    "        # Parse all metadata\n",
    "        modality = int(parts[0])\n",
    "        vocal_channel = int(parts[1])\n",
    "        emotion = int(parts[2]) - 1\n",
    "        intensity = int(parts[3])\n",
    "        statement = int(parts[4])\n",
    "        repetition = int(parts[5])\n",
    "        actor = int(parts[6].split(\".\")[0])\n",
    "        gender = 0 if actor % 2 == 1 else 1\n",
    "\n",
    "        # Load and process audio\n",
    "        waveform, sample_rate = torchaudio.load(audio_path, format=\"wav\")\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.augment:\n",
    "            waveform = self.apply_augmentation(waveform, sample_rate)\n",
    "\n",
    "        # Normalize the waveform (zero mean, unit variance)\n",
    "        waveform = (waveform - waveform.mean()) / (waveform.std() + 1e-7)\n",
    "        \n",
    "        # Resample to 16kHz if needed\n",
    "        if sample_rate != 16000:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "            waveform = resampler(waveform)\n",
    "        \n",
    "        # Pad or truncate to target length\n",
    "        if waveform.shape[1] < self.target_length:\n",
    "            # Pad with zeros\n",
    "            pad_amount = self.target_length - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_amount))\n",
    "        elif waveform.shape[1] > self.target_length:\n",
    "            # Random crop\n",
    "            start = torch.randint(0, waveform.shape[1] - self.target_length, (1,)).item()\n",
    "            waveform = waveform[:, start:start+self.target_length]\n",
    "        \n",
    "        # Convert to mono if needed\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        # Generate Mel spectrogram\n",
    "        mel_spec = self.mel_spectrogram(waveform)\n",
    "        \n",
    "        # Apply log compression\n",
    "        log_mel_spec = self.amplitude_to_db(mel_spec)\n",
    "        \n",
    "        # Normalize spectrogram to [0, 1] range\n",
    "        log_mel_spec = (log_mel_spec - log_mel_spec.min()) / (log_mel_spec.max() - log_mel_spec.min())\n",
    "        \n",
    "        # Resize spectrogram to 224x224\n",
    "        resized_spec = torch.nn.functional.interpolate(\n",
    "            log_mel_spec.unsqueeze(0), size=(224, 224), mode=\"bilinear\", align_corners=False\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # Convert to 3-channel \"image\" by repeating the spectrogram\n",
    "        # (ViT typically expects 3-channel input)\n",
    "        spectrogram_img = resized_spec.repeat(3, 1, 1)\n",
    "        \n",
    "        label = {\n",
    "            \"modality\": modality,\n",
    "            \"vocal_channel\": vocal_channel,\n",
    "            \"emotion\": emotion,\n",
    "            \"intensity\": intensity,\n",
    "            \"statement\": statement,\n",
    "            \"repetition\": repetition,\n",
    "            \"gender\": gender,\n",
    "        }\n",
    "\n",
    "        return spectrogram_img, label['emotion'], label\n",
    "\n",
    "    def apply_augmentation(self, waveform, sample_rate):\n",
    "        \"\"\"\n",
    "        Apply audio augmentations suitable for spectrogram generation.\n",
    "        \"\"\"\n",
    "        # Resample to 16kHz\n",
    "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
    "\n",
    "        # Apply time masking\n",
    "        waveform = torchaudio.transforms.TimeMasking(time_mask_param=100)(waveform)\n",
    "\n",
    "        # Apply frequency masking\n",
    "        waveform = torchaudio.transforms.FrequencyMasking(freq_mask_param=15)(waveform)\n",
    "\n",
    "        # Adjust volume\n",
    "        waveform = waveform * 0.5  # Equivalent to T.Vol(0.5)\n",
    "\n",
    "        return waveform\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for spectrogram data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spectrograms, emotions, labels = zip(*batch)\n",
    "        \n",
    "        # Stack spectrograms\n",
    "        spectrograms = torch.stack(spectrograms)\n",
    "        \n",
    "        # Stack emotions\n",
    "        emotions = torch.tensor(emotions)\n",
    "        \n",
    "        return {\n",
    "            'spectrograms': spectrograms,\n",
    "            'emotions': emotions,\n",
    "            'metadata': labels\n",
    "        }\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_ravdess_dataset(base_dir, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load the RAVDESS dataset and split it into training and testing datasets.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for actor_folder in os.listdir(base_dir):\n",
    "        actor_path = os.path.join(base_dir, actor_folder)\n",
    "        if os.path.isdir(actor_path):\n",
    "            for audio_file in os.listdir(actor_path):\n",
    "                if audio_file.endswith(\".wav\"):\n",
    "                    parts = audio_file.split(\"-\")\n",
    "                    emotion = int(parts[2])\n",
    "                    audio_path = os.path.join(actor_path, audio_file)\n",
    "                    data.append((audio_path, emotion))\n",
    "\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    train_dataset = AudioDataset(train_data)\n",
    "    test_dataset = AudioDataset(test_data)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_data_loaders(base_dir, batch_size=8, test_size=0.2, random_state=42, augment=False):\n",
    "    train_dataset, test_dataset = load_ravdess_dataset(base_dir, test_size, random_state)\n",
    "\n",
    "    train_dataset.augment = augment\n",
    "\n",
    "    def safe_collate(batch):\n",
    "        batch = [item for item in batch if item is not None]\n",
    "        return collate_fn(batch)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=safe_collate\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=safe_collate\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d35fc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_data_loaders(\"ravdess_dataset\", batch_size=2, augment=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a489cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gnith\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "vit = timm.create_model(\n",
    "            'vit_base_patch16_224',\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # We'll add our own head\n",
    "            in_chans=3  # Our spectrograms have 3 channels\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0e199a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_token\n",
      "pos_embed\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.attn.qkv.weight\n",
      "blocks.0.attn.qkv.bias\n",
      "blocks.0.attn.proj.weight\n",
      "blocks.0.attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.attn.qkv.weight\n",
      "blocks.1.attn.qkv.bias\n",
      "blocks.1.attn.proj.weight\n",
      "blocks.1.attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.attn.qkv.weight\n",
      "blocks.2.attn.qkv.bias\n",
      "blocks.2.attn.proj.weight\n",
      "blocks.2.attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.attn.qkv.weight\n",
      "blocks.3.attn.qkv.bias\n",
      "blocks.3.attn.proj.weight\n",
      "blocks.3.attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "blocks.4.norm1.weight\n",
      "blocks.4.norm1.bias\n",
      "blocks.4.attn.qkv.weight\n",
      "blocks.4.attn.qkv.bias\n",
      "blocks.4.attn.proj.weight\n",
      "blocks.4.attn.proj.bias\n",
      "blocks.4.norm2.weight\n",
      "blocks.4.norm2.bias\n",
      "blocks.4.mlp.fc1.weight\n",
      "blocks.4.mlp.fc1.bias\n",
      "blocks.4.mlp.fc2.weight\n",
      "blocks.4.mlp.fc2.bias\n",
      "blocks.5.norm1.weight\n",
      "blocks.5.norm1.bias\n",
      "blocks.5.attn.qkv.weight\n",
      "blocks.5.attn.qkv.bias\n",
      "blocks.5.attn.proj.weight\n",
      "blocks.5.attn.proj.bias\n",
      "blocks.5.norm2.weight\n",
      "blocks.5.norm2.bias\n",
      "blocks.5.mlp.fc1.weight\n",
      "blocks.5.mlp.fc1.bias\n",
      "blocks.5.mlp.fc2.weight\n",
      "blocks.5.mlp.fc2.bias\n",
      "blocks.6.norm1.weight\n",
      "blocks.6.norm1.bias\n",
      "blocks.6.attn.qkv.weight\n",
      "blocks.6.attn.qkv.bias\n",
      "blocks.6.attn.proj.weight\n",
      "blocks.6.attn.proj.bias\n",
      "blocks.6.norm2.weight\n",
      "blocks.6.norm2.bias\n",
      "blocks.6.mlp.fc1.weight\n",
      "blocks.6.mlp.fc1.bias\n",
      "blocks.6.mlp.fc2.weight\n",
      "blocks.6.mlp.fc2.bias\n",
      "blocks.7.norm1.weight\n",
      "blocks.7.norm1.bias\n",
      "blocks.7.attn.qkv.weight\n",
      "blocks.7.attn.qkv.bias\n",
      "blocks.7.attn.proj.weight\n",
      "blocks.7.attn.proj.bias\n",
      "blocks.7.norm2.weight\n",
      "blocks.7.norm2.bias\n",
      "blocks.7.mlp.fc1.weight\n",
      "blocks.7.mlp.fc1.bias\n",
      "blocks.7.mlp.fc2.weight\n",
      "blocks.7.mlp.fc2.bias\n",
      "blocks.8.norm1.weight\n",
      "blocks.8.norm1.bias\n",
      "blocks.8.attn.qkv.weight\n",
      "blocks.8.attn.qkv.bias\n",
      "blocks.8.attn.proj.weight\n",
      "blocks.8.attn.proj.bias\n",
      "blocks.8.norm2.weight\n",
      "blocks.8.norm2.bias\n",
      "blocks.8.mlp.fc1.weight\n",
      "blocks.8.mlp.fc1.bias\n",
      "blocks.8.mlp.fc2.weight\n",
      "blocks.8.mlp.fc2.bias\n",
      "blocks.9.norm1.weight\n",
      "blocks.9.norm1.bias\n",
      "blocks.9.attn.qkv.weight\n",
      "blocks.9.attn.qkv.bias\n",
      "blocks.9.attn.proj.weight\n",
      "blocks.9.attn.proj.bias\n",
      "blocks.9.norm2.weight\n",
      "blocks.9.norm2.bias\n",
      "blocks.9.mlp.fc1.weight\n",
      "blocks.9.mlp.fc1.bias\n",
      "blocks.9.mlp.fc2.weight\n",
      "blocks.9.mlp.fc2.bias\n",
      "blocks.10.norm1.weight\n",
      "blocks.10.norm1.bias\n",
      "blocks.10.attn.qkv.weight\n",
      "blocks.10.attn.qkv.bias\n",
      "blocks.10.attn.proj.weight\n",
      "blocks.10.attn.proj.bias\n",
      "blocks.10.norm2.weight\n",
      "blocks.10.norm2.bias\n",
      "blocks.10.mlp.fc1.weight\n",
      "blocks.10.mlp.fc1.bias\n",
      "blocks.10.mlp.fc2.weight\n",
      "blocks.10.mlp.fc2.bias\n",
      "blocks.11.norm1.weight\n",
      "blocks.11.norm1.bias\n",
      "blocks.11.attn.qkv.weight\n",
      "blocks.11.attn.qkv.bias\n",
      "blocks.11.attn.proj.weight\n",
      "blocks.11.attn.proj.bias\n",
      "blocks.11.norm2.weight\n",
      "blocks.11.norm2.bias\n",
      "blocks.11.mlp.fc1.weight\n",
      "blocks.11.mlp.fc1.bias\n",
      "blocks.11.mlp.fc2.weight\n",
      "blocks.11.mlp.fc2.bias\n",
      "norm.weight\n",
      "norm.bias\n"
     ]
    }
   ],
   "source": [
    "for a,b in vit.named_parameters():\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce08d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
