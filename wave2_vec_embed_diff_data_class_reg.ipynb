{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchaudio\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np \n",
    "from torch.utils.data import DataLoader,Dataset,random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoProcessor, Wav2Vec2Model\n",
    "import torch as t\n",
    "import numpy as np\n",
    "t.cuda.set_device(8)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir = \"/raid/amana/lavish_multi_model/emotion_detection/data/raw_audio/\"\n",
    "base_path = \"/raid/amana/lavish_multi_model/emotion_detection/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Oh my God, hes lost it. Hes totally lost it.</td>\n",
       "      <td>Phoebe</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:20:57,256</td>\n",
       "      <td>00:21:00,049</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>What?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>surprise</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>00:21:01,927</td>\n",
       "      <td>00:21:03,261</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Or! Or, we could go to the bank, close our acc...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:24,660</td>\n",
       "      <td>00:12:30,915</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Youre a genius!</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:32,334</td>\n",
       "      <td>00:12:33,960</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Aww, man, now we wont be bank buddies!</td>\n",
       "      <td>Joey</td>\n",
       "      <td>sadness</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:34,211</td>\n",
       "      <td>00:12:37,505</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance   Speaker  \\\n",
       "0       1     Oh my God, hes lost it. Hes totally lost it.    Phoebe   \n",
       "1       2                                              What?    Monica   \n",
       "2       3  Or! Or, we could go to the bank, close our acc...      Ross   \n",
       "3       4                                   Youre a genius!  Chandler   \n",
       "4       5            Aww, man, now we wont be bank buddies!      Joey   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   sadness  negative            0             0       4        7   \n",
       "1  surprise  negative            0             1       4        7   \n",
       "2   neutral   neutral            1             0       4        4   \n",
       "3       joy  positive            1             1       4        4   \n",
       "4   sadness  negative            1             2       4        4   \n",
       "\n",
       "      StartTime       EndTime  \\\n",
       "0  00:20:57,256  00:21:00,049   \n",
       "1  00:21:01,927  00:21:03,261   \n",
       "2  00:12:24,660  00:12:30,915   \n",
       "3  00:12:32,334  00:12:33,960   \n",
       "4  00:12:34,211  00:12:37,505   \n",
       "\n",
       "                                            filename  \n",
       "0  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "1  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "2  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "3  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "4  /raid/amana/lavish_multi_model/emotion_detecti...  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/raid/amana/lavish_multi_model/emotion_detection/data/text_data.csv\")\n",
    "df['filename'] = [f'{audio_dir}dia{a}_utt{b}.wav' for a,b in zip(df['Dialogue_ID'],df['Utterance_ID'])]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: 1109\n",
      "Step 1: 1109\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sr No.</th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Now, theres two reasons.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>00:12:38,466</td>\n",
       "      <td>00:12:39,841</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1119</td>\n",
       "      <td>Ugh, how can you even ask that question?!</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0:02:45,501</td>\n",
       "      <td>0:02:48,182</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>758</td>\n",
       "      <td>Yeah! You can hook it up to your TV</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>71</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>00:13:26,514</td>\n",
       "      <td>00:13:29,015</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>Come on man!  Listen so uh, are you gonna sque...</td>\n",
       "      <td>Joey</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>00:07:01,254</td>\n",
       "      <td>00:07:07,342</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1107</td>\n",
       "      <td>Yknow what Greg?</td>\n",
       "      <td>Monica</td>\n",
       "      <td>anger</td>\n",
       "      <td>negative</td>\n",
       "      <td>105</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>00:22:03,488</td>\n",
       "      <td>00:22:04,947</td>\n",
       "      <td>/raid/amana/lavish_multi_model/emotion_detecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sr No.                                          Utterance   Speaker  \\\n",
       "0       6                          Now, theres two reasons.  Chandler   \n",
       "1    1119          Ugh, how can you even ask that question?!    Rachel   \n",
       "2     758                Yeah! You can hook it up to your TV    Rachel   \n",
       "3     104  Come on man!  Listen so uh, are you gonna sque...      Joey   \n",
       "4    1107                                  Yknow what Greg?    Monica   \n",
       "\n",
       "    Emotion Sentiment  Dialogue_ID  Utterance_ID  Season  Episode  \\\n",
       "0   neutral   neutral            1             3       4        4   \n",
       "1  surprise  positive          107             3       6       20   \n",
       "2       joy  positive           71             4       7       13   \n",
       "3       joy  positive           10             3       5       20   \n",
       "4     anger  negative          105            10       8        4   \n",
       "\n",
       "      StartTime       EndTime  \\\n",
       "0  00:12:38,466  00:12:39,841   \n",
       "1   0:02:45,501   0:02:48,182   \n",
       "2  00:13:26,514  00:13:29,015   \n",
       "3  00:07:01,254  00:07:07,342   \n",
       "4  00:22:03,488  00:22:04,947   \n",
       "\n",
       "                                            filename  \n",
       "0  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "1  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "2  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "3  /raid/amana/lavish_multi_model/emotion_detecti...  \n",
       "4  /raid/amana/lavish_multi_model/emotion_detecti...  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Step 0: {len(df)}\")\n",
    "df[\"status\"] = df[\"filename\"].apply(lambda path: True if os.path.exists(path) else None)\n",
    "df = df.dropna(subset=[\"filename\"])\n",
    "df = df.drop(\"status\", 1)\n",
    "print(f\"Step 1: {len(df)}\")\n",
    "df = df.sample(frac=1)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['neutral' 'surprise' 'joy' 'anger' 'fear' 'sadness' 'disgust']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emotion</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anger</th>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disgust</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fear</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>joy</th>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadness</th>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surprise</th>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename\n",
       "Emotion           \n",
       "anger          153\n",
       "disgust         22\n",
       "fear            40\n",
       "joy            163\n",
       "neutral        470\n",
       "sadness        111\n",
       "surprise       150"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Labels: \", df[\"Emotion\"].unique())\n",
    "print()\n",
    "df.groupby(\"Emotion\").count()[[\"filename\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 13)\n",
      "(222, 13)\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/raid/amana/lavish_multi_model/emotion_detection/data/\"\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=101, stratify=df[\"Emotion\"])\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "random_float_array = np.random.uniform(7, 10.0, len(train_df.Emotion))\n",
    "train_df[\"emotion\"] = train_df['Emotion']\n",
    "train_df['Emotion'] = random_float_array.round(2)\n",
    "train_df = train_df.rename(columns={\"Emotion\":\"values\"})\n",
    "\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "random_float_array = np.random.uniform(7, 10.0, len(test_df.Emotion))\n",
    "test_df[\"emotion\"] = test_df['Emotion']\n",
    "test_df['Emotion'] = random_float_array.round(2)\n",
    "test_df = test_df.rename(columns={\"Emotion\":\"values\"})\n",
    "\n",
    "train_df.to_csv(f\"{save_path}/train.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "test_df.to_csv(f\"{save_path}/test.csv\", sep=\"\\t\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2Model\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import soundfile as sf\n",
    "import librosa\n",
    "\n",
    "\n",
    "def getVggoud_proc(filename):\n",
    "    audio_length = 1.5\n",
    "    idx=0\n",
    "    samples, samplerate = librosa.load(filename)\n",
    "\n",
    "    if samples.shape[0] > 16000*(audio_length+0.1):\n",
    "        sample_indx = np.linspace(0, samples.shape[0]-16000*(audio_length+0.1), num=1, dtype=int)\n",
    "        samples = samples[sample_indx[idx]:sample_indx[idx]+int(16000*audio_length)]\n",
    "    else:\n",
    "        samples = np.tile(samples,int(audio_length))[:int(16000*audio_length)]\n",
    "\n",
    "    samples[samples > 1.] = 1\n",
    "    samples[samples < -1.] = -1\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def resample_audio(arr,sr):\n",
    "    return librosa.util.fix_length(arr, size=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_classification(df):\n",
    "    emotion_label = {'anger' : 1, 'disgust' : 2, 'fear' : 3,\n",
    "                 'joy' : 4, 'neutral' : 5, 'sadness' : 6, 'surprise' : 0}\n",
    "    labels = list(df['emotion'])\n",
    "    onlyfiles = list(df['filename'])\n",
    "    res = []\n",
    "    shapes = []\n",
    "    for a,label in zip(onlyfiles,labels):\n",
    "        try:\n",
    "            res1 = getVggoud_proc(a)\n",
    "            shapes.append(res1.shape)\n",
    "            res.append([res1,label])\n",
    "        except:\n",
    "            continue\n",
    "    shapes = np.array(shapes)\n",
    "    true_shapes = shapes>5000\n",
    "    res = [a for a,b in zip(res,true_shapes) if b[0] == True]\n",
    "    final_len = 16000\n",
    "    final_dataset = [[resample_audio(a[0],final_len),emotion_label[a[1]]] for a in res]\n",
    "    return final_dataset\n",
    "\n",
    "def get_dataset_regression(df):\n",
    "    labels = list(df['values'])\n",
    "    onlyfiles = list(df['filename'])\n",
    "    res = []\n",
    "    shapes = []\n",
    "    for a,label in zip(onlyfiles,labels):\n",
    "        try:\n",
    "            res1 = getVggoud_proc(a)\n",
    "            shapes.append(res1.shape)\n",
    "            res.append([res1,label])\n",
    "        except:\n",
    "            continue\n",
    "    shapes = np.array(shapes)\n",
    "    true_shapes = shapes>5000\n",
    "    res = [a for a,b in zip(res,true_shapes) if b[0] == True]\n",
    "    final_len = 16000\n",
    "    final_dataset = [[resample_audio(a[0],final_len),a[1]] for a in res]\n",
    "    return final_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      7.91\n",
       "1      7.45\n",
       "2      8.39\n",
       "3      8.06\n",
       "4      8.37\n",
       "       ... \n",
       "217    7.96\n",
       "218    9.39\n",
       "219    7.67\n",
       "220    8.39\n",
       "221    7.81\n",
       "Name: values, Length: 222, dtype: float64"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset_regression(train_df)\n",
    "test_dataset = get_dataset_regression(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2Model: ['lm_head.bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, Wav2Vec2Model\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_dataset(dataset):\n",
    "    final_dataset = []\n",
    "    for a in dataset:\n",
    "        inputs = processor(a[0], sampling_rate=16000, return_tensors=\"pt\").to(device)\n",
    "        with t.no_grad():\n",
    "            outputs = model(**inputs).extract_features\n",
    "            final_dataset.append([outputs.squeeze(0),a[1]])\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset1 = get_final_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = get_final_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset1, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset1, batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(train_dataloader.dataset, 'wave2_vec_diff_reg_data_train_dataset.pth')\n",
    "\n",
    "# Save the testing DataLoader\n",
    "t.save(test_dataloader.dataset, 'wave2_vec_diff_reg_data_test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.cuda.set_device(8)\n",
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionMLP_cls(nn.Module):\n",
    "    def softmax(self,x):\n",
    "        e_x = t.exp(x - t.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def __init__(self, num_classes):\n",
    "        super(EmotionMLP_cls, self).__init__()\n",
    "        self.input_matrix = nn.Parameter(t.rand(49,1))\n",
    "        self.fc1 = nn.Linear(512,64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = t.matmul(x.t(),self.input_matrix)\n",
    "        x = x.view(-1, 512)\n",
    "        x = t.relu(self.fc1(x))\n",
    "        x = t.relu(self.fc2(x))\n",
    "        x = t.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "class EmotionMLP_reg(nn.Module):\n",
    "    def softmax(self,x):\n",
    "        e_x = t.exp(x - t.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(EmotionMLP_reg, self).__init__()\n",
    "        self.input_matrix = nn.Parameter(t.rand(49,1))\n",
    "        self.fc1 = nn.Linear(512,32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = t.matmul(x.t(),self.input_matrix)\n",
    "        x = x.view(-1, 512)\n",
    "        x = t.relu(self.fc1(x))\n",
    "        x = t.relu(self.fc2(x)).double()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]], device='cuda:8', dtype=torch.float64, grad_fn=<CopyBackwards>)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmotionMLP_reg().to(device)\n",
    "\n",
    "x = t.rand(49,512)\n",
    "model(x.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmotionMLP_reg().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = t.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(lossarr,epochs):\n",
    "    plt.plot(range(1,epochs+1),lossarr)\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,test_loader,device):\n",
    "    \n",
    "    def accuracy1(y_true, y_pred):\n",
    "        eq = t.eq(y_true, y_pred).int()\n",
    "        return sum(eq)/len(eq)\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        # print(y_true,y_pred)\n",
    "        return t.mean(t.abs((y_true - y_pred) / y_true))*100\n",
    "    \n",
    "    acc = 0\n",
    "    mape = 0\n",
    "    with t.no_grad():\n",
    "        model.eval()\n",
    "        for x,y in test_loader:\n",
    "            x = x.squeeze(0)\n",
    "            outputs = model(x.to(device))\n",
    "            outputs1 = outputs.detach().cpu()\n",
    "            mape += mean_absolute_percentage_error(t.tensor([y]),outputs1)\n",
    "        print(f\"mape: {((mape/len(test_loader)))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Training Loss: 4.437607669340425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:05<04:19,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 10.24863724485304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 3/50 [00:14<03:46,  4.82s/it]"
     ]
    }
   ],
   "source": [
    "def train(model,train_loader,optimizer,criterion,num_epochs,device):\n",
    "    loss_arr = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        # model.train()\n",
    "        total_loss = 0\n",
    "        for x,y in train_loader:\n",
    "            x = x.squeeze(0)\n",
    "            # Forward pass\n",
    "            outputs = model(x.to(device))\n",
    "            loss = criterion(outputs,t.tensor([y],dtype=float).to(device)).to(device)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        loss_arr.append(total_loss/len(train_loader))\n",
    "        \n",
    "        if epoch%5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss/len(train_loader)}')\n",
    "            test(model,test_dataloader,device)\n",
    "    return loss_arr\n",
    "\n",
    "num_epochs = 50\n",
    "lossarr = train(model,train_dataloader,optimizer,criterion,num_epochs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss(lossarr,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def test(model,test_loader,device):\n",
    "    \n",
    "    def accuracy1(y_true, y_pred):\n",
    "        eq = t.eq(y_true, y_pred).int()\n",
    "        return sum(eq)/len(eq)\n",
    "    def mean_absolute_percentage_error(y_true, y_pred):\n",
    "        # print(y_true,y_pred)\n",
    "        return t.mean(t.abs((y_true - y_pred) / y_true))*100\n",
    "    \n",
    "    def MAE(y_true,y_pred):\n",
    "        y_true = y_true.detach().numpy()\n",
    "        y_pred = y_pred.detach().numpy()\n",
    "        return mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "    \n",
    "    acc = 0\n",
    "    mape = 0\n",
    "    mae = 0\n",
    "    with t.no_grad():\n",
    "        model.eval()\n",
    "        for x,y in test_loader:\n",
    "            x = x.squeeze(0)\n",
    "            outputs = model(x.to(device))\n",
    "            outputs1 = outputs.detach().cpu()\n",
    "            mape += mean_absolute_percentage_error(t.tensor([y]),outputs1)\n",
    "            mae += MAE(t.tensor([y]),outputs1)\n",
    "        print(f\"mape: {((mape/len(test_loader))) :0.2f}\")\n",
    "        print(f\"mae: {((mae/len(test_loader))) :0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mape: 22.95\n",
      "mae: 1.98\n"
     ]
    }
   ],
   "source": [
    "test(model,test_dataloader,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lamol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
